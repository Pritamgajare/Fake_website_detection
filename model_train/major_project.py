# -*- coding: utf-8 -*-
"""Major_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1flxiI5ejL3Joo2WrJUiL25z82wVB36Zx
"""

#importing basic packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#Loading the data
data0 = pd.read_csv('final_dataset.csv')
data0.head()

#Checking the shape of the dataset
data0.shape

#Listing the features of the dataset
data0.columns

#Information about the dataset
data0.info()

#Plotting the data distribution
data0.hist(bins = 50,figsize = (15,15))
plt.show()

data0.describe()

# data = data0.copy()
#Dropping the Domain column
data = data0.drop(['Domain'], axis = 1).copy()

#checking the data for null or missing values
data.isnull().sum()

# shuffling the rows in the dataset so that when splitting the train and test set are equally distributed
data = data.sample(frac=1).reset_index(drop=True)
data.head()

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from keras.models import Sequential
from keras.layers import SimpleRNN, LSTM, Dense

# Separate features and target
y = data['Label']
X = data.drop('Label', axis=1)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)

# Reshape input data for RNN/LSTM
X_train_reshaped = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))
X_test_reshaped = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))

# Function to store results
ML_Model = []
acc_train = []
acc_test = []
def storeResults(model, a, b):
    ML_Model.append(model)
    acc_train.append(round(a, 3))
    acc_test.append(round(b, 3))

# RNN Model
rnn_model = Sequential()
rnn_model.add(SimpleRNN(50, activation='relu', input_shape=(1, X_train.shape[1])))
rnn_model.add(Dense(1, activation='sigmoid'))
rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_rnn = rnn_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=64, validation_split=0.2)
acc_train_rnn = rnn_model.evaluate(X_train_reshaped, y_train)[1]
acc_test_rnn = rnn_model.evaluate(X_test_reshaped, y_test)[1]
print(f'RNN: Accuracy on training Data: {acc_train_rnn:.3f}')
print(f'RNN: Accuracy on test Data: {acc_test_rnn:.3f}')
storeResults('RNN', acc_train_rnn, acc_test_rnn)

# LSTM Model
lstm_model = Sequential()
lstm_model.add(LSTM(50, activation='relu', input_shape=(1, X_train.shape[1])))
lstm_model.add(Dense(1, activation='sigmoid'))
lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_lstm = lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=64, validation_split=0.2)
acc_train_lstm = lstm_model.evaluate(X_train_reshaped, y_train)[1]
acc_test_lstm = lstm_model.evaluate(X_test_reshaped, y_test)[1]
print(f'LSTM: Accuracy on training Data: {acc_train_lstm:.3f}')
print(f'LSTM: Accuracy on test Data: {acc_test_lstm:.3f}')
storeResults('LSTM', acc_train_lstm, acc_test_lstm)

from keras.models import Sequential
from keras.layers import LSTM, Dense, Bidirectional

# Bidirectional LSTM Model
bidirectional_lstm_model = Sequential()
bidirectional_lstm_model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(1, X_train.shape[1])))
bidirectional_lstm_model.add(Dense(1, activation='sigmoid'))
bidirectional_lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history_bidirectional_lstm = bidirectional_lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=64, validation_split=0.2)
acc_train_bidirectional_lstm = bidirectional_lstm_model.evaluate(X_train_reshaped, y_train)[1]
acc_test_bidirectional_lstm = bidirectional_lstm_model.evaluate(X_test_reshaped, y_test)[1]

print(f'Bidirectional LSTM: Accuracy on training Data: {acc_train_bidirectional_lstm:.3f}')
print(f'Bidirectional LSTM: Accuracy on test Data: {acc_test_bidirectional_lstm:.3f}')

storeResults('Bidirectional LSTM', acc_train_bidirectional_lstm, acc_test_bidirectional_lstm)

from keras.models import Sequential
from keras.layers import LSTM, Dense

# Stacked LSTM Model
stacked_lstm_model = Sequential()
stacked_lstm_model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(1, X_train.shape[1])))
stacked_lstm_model.add(LSTM(50, activation='relu'))
stacked_lstm_model.add(Dense(1, activation='sigmoid'))
stacked_lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history_stacked_lstm = stacked_lstm_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=64, validation_split=0.2)
acc_train_stacked_lstm = stacked_lstm_model.evaluate(X_train_reshaped, y_train)[1]
acc_test_stacked_lstm = stacked_lstm_model.evaluate(X_test_reshaped, y_test)[1]

print(f'Stacked LSTM: Accuracy on training Data: {acc_train_stacked_lstm:.3f}')
print(f'Stacked LSTM: Accuracy on test Data: {acc_test_stacked_lstm:.3f}')

storeResults('Stacked LSTM', acc_train_stacked_lstm, acc_test_stacked_lstm)

from keras.models import Sequential
from keras.layers import GRU, Dense

# GRU Model
gru_model = Sequential()
gru_model.add(GRU(50, activation='relu', input_shape=(1, X_train.shape[1])))
gru_model.add(Dense(1, activation='sigmoid'))
gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history_gru = gru_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=64, validation_split=0.2)
acc_train_gru = gru_model.evaluate(X_train_reshaped, y_train)[1]
acc_test_gru = gru_model.evaluate(X_test_reshaped, y_test)[1]

print(f'GRU: Accuracy on training Data: {acc_train_gru:.3f}')
print(f'GRU: Accuracy on test Data: {acc_test_gru:.3f}')

storeResults('GRU', acc_train_gru, acc_test_gru)

from keras.layers import Input, LSTM, Dense, Attention
from keras.models import Model

# Input layer
inputs = Input(shape=(X_train.shape[1], 1))

# LSTM layer
lstm_out, state_h, state_c = LSTM(50, return_sequences=True, return_state=True)(inputs)

# Attention mechanism
attention = Attention()([lstm_out, lstm_out])
context_vector = Dense(50, activation='relu')(attention)

# Output layer
outputs = Dense(1, activation='sigmoid')(context_vector)

# Define the model
attention_model = Model(inputs=inputs, outputs=outputs)

# Compile the model
attention_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Convert DataFrame to numpy array
X_train_array = X_train.values
X_test_array = X_test.values

# Reshape data for the model
X_train_reshaped = X_train_array.reshape((X_train_array.shape[0], X_train_array.shape[1], 1))
X_test_reshaped = X_test_array.reshape((X_test_array.shape[0], X_test_array.shape[1], 1))


# Train the model
history_attention = attention_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=64, validation_split=0.2)

# Evaluate the model
acc_train_attention = attention_model.evaluate(X_train_reshaped, y_train)[1]
acc_test_attention = attention_model.evaluate(X_test_reshaped, y_test)[1]

print(f'Attention Model: Accuracy on training Data: {acc_train_attention:.3f}')
print(f'Attention Model: Accuracy on test Data: {acc_test_attention:.3f}')

storeResults('Attention Model', acc_train_attention, acc_test_attention)

from keras.layers import Input, LSTM, Dense
from keras.models import Model

# Encoder
encoder_inputs = Input(shape=(X_train.shape[1], 1))
encoder_lstm = LSTM(50, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = Input(shape=(None, 1))
decoder_lstm = LSTM(50, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = Dense(1, activation='sigmoid')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model
seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile the model
seq2seq_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Convert DataFrame to numpy array
X_train_array = X_train.values
X_test_array = X_test.values

# Reshape data for the model
X_train_reshaped = X_train_array.reshape((X_train_array.shape[0], X_train_array.shape[1], 1))
X_test_reshaped = X_test_array.reshape((X_test_array.shape[0], X_test_array.shape[1], 1))


# Create decoder inputs (we need to provide them during training)
decoder_input_data = np.zeros_like(X_train_reshaped)

# Train the model
history_seq2seq = seq2seq_model.fit([X_train_reshaped, decoder_input_data], X_train_reshaped, epochs=10, batch_size=64, validation_split=0.2)

# Evaluate the model
acc_train_seq2seq = seq2seq_model.evaluate([X_train_reshaped, decoder_input_data], X_train_reshaped)[1]
acc_test_seq2seq = seq2seq_model.evaluate([X_test_reshaped, np.zeros_like(X_test_reshaped)], X_test_reshaped)[1]

print(f'Seq2Seq Model: Accuracy on training Data: {acc_train_seq2seq:.3f}')
print(f'Seq2Seq Model: Accuracy on test Data: {acc_test_seq2seq:.3f}')

storeResults('Seq2Seq Model', acc_train_seq2seq, acc_test_seq2seq)

from keras.layers import Input, LSTM, Dense, TimeDistributed
from keras.models import Model

# Input layer
inputs = Input(shape=(X_train.shape[1], 1))

# LSTM layer
lstm_out = LSTM(50, return_sequences=True)(inputs)

# TimeDistributed layer
time_distributed_out = TimeDistributed(Dense(50, activation='relu'))(lstm_out)

# Output layer
outputs = TimeDistributed(Dense(1, activation='sigmoid'))(time_distributed_out)

# Define the model
time_distributed_model = Model(inputs=inputs, outputs=outputs)

# Compile the model
time_distributed_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Convert DataFrame to numpy array
X_train_array = X_train.values
X_test_array = X_test.values

# Reshape data for the model
X_train_reshaped = X_train_array.reshape((X_train_array.shape[0], X_train_array.shape[1], 1))
X_test_reshaped = X_test_array.reshape((X_test_array.shape[0], X_test_array.shape[1], 1))


# Train the model
history_time_distributed = time_distributed_model.fit(X_train_reshaped, X_train_reshaped, epochs=10, batch_size=64, validation_split=0.2)

# Evaluate the model
acc_train_time_distributed = time_distributed_model.evaluate(X_train_reshaped, X_train_reshaped)[1]
acc_test_time_distributed = time_distributed_model.evaluate(X_test_reshaped, X_test_reshaped)[1]

print(f'Time Distributed Model: Accuracy on training Data: {acc_train_time_distributed:.3f}')
print(f'Time Distributed Model: Accuracy on test Data: {acc_test_time_distributed:.3f}')

storeResults('Time Distributed Model', acc_train_time_distributed, acc_test_time_distributed)

# Create dataframe for results
results = pd.DataFrame({
    'ML Model': ML_Model,
    'Train Accuracy': acc_train,
    'Test Accuracy': acc_test
})
results

results.sort_values(by=['Test Accuracy', 'Train Accuracy'], ascending=False)

# # Save the model
# time_distributed_model.save('timedistributed_lstm_model.h5')
# print("Model saved as 'timedistributed_lstm_model.h5'")

# seq2seq_model.save('seq2seq_model.h5')
# print("Model saved as 'seq2seq_model.h5'")

# Load the model
# from keras.models import load_model
# loaded_model = load_model('seq2seq_model.h5')
# print("Model loaded successfully")

# pip install python-whois

# -*- coding: utf-8 -*-

# importing required packages for this section
from urllib.parse import urlparse, urlencode
import ipaddress
import re
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import whois
from bs4 import BeautifulSoup
import requests
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import load_model

# Define the functions as given

def havingIP(url):
    try:
        ipaddress.ip_address(url)
        ip = 1
    except:
        ip = 0
    return ip

def haveAtSign(url):
    return 1 if "@" in url else 0

def getLength(url):
    return 1 if len(url) >= 54 else 0

def getDepth(url):
    s = urlparse(url).path.split('/')
    depth = 0
    for j in range(len(s)):
        if len(s[j]) != 0:
            depth += 1
    return depth

def redirection(url):
    pos = url.rfind('//')
    if pos > 6:
        return 1 if pos > 7 else 0
    else:
        return 0

def httpDomain(url):
    domain = urlparse(url).netloc
    return 1 if 'https' in domain else 0

shortening_services = r"bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|lnkd\.in|db\.tt|qr\.ae|adf\.ly|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|ity\.im|q\.gs|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|tr\.im|link\.zip\.net"

def tinyURL(url):
    return 1 if re.search(shortening_services, url) else 0

def prefixSuffix(url):
    return 1 if '-' in urlparse(url).netloc else 0

def web_traffic(url):
    try:
        rank = BeautifulSoup(urllib.request.urlopen("http://data.alexa.com/data?cli=10&dat=s&url=" + url, timeout=10).read(), "xml").find("REACH")['RANK']
        rank = int(rank)
    except Exception as e:
        return 1
    return 1 if rank < 100000 else 0

def domainAge(domain_name):
    try:
        creation_date = domain_name.creation_date
        expiration_date = domain_name.expiration_date
        if (isinstance(creation_date, str) or isinstance(expiration_date, str)):
            creation_date = datetime.strptime(creation_date, '%Y-%m-%d')
            expiration_date = datetime.strptime(expiration_date, "%Y-%m-%d")
        if (expiration_date is None) or (creation_date is None):
            return 1
        ageofdomain = abs((expiration_date - creation_date).days)
        return 1 if ((ageofdomain / 30) < 6) else 0
    except:
        return 1

def domainEnd(domain_name):
    try:
        expiration_date = domain_name.expiration_date
        if isinstance(expiration_date, str):
            expiration_date = datetime.strptime(expiration_date, "%Y-%m-%d")
        if expiration_date is None:
            return 1
        today = datetime.now()
        end = abs((expiration_date - today).days)
        return 0 if ((end / 30) < 6) else 1
    except:
        return 1

def iframe(response):
    try:
        return 0 if re.findall(r"[<iframe>|<frameBorder>]", response.text) else 1
    except:
        return 1

def mouseOver(response):
    try:
        return 1 if re.findall("<script>.+onmouseover.+</script>", response.text) else 0
    except:
        return 1

def rightClick(response):
    try:
        return 0 if re.findall(r"event.button ?== ?2", response.text) else 1
    except:
        return 1

def forwarding(response):
    try:
        return 0 if len(response.history) <= 2 else 1
    except:
        return 1

def featureExtraction(url):
    features = []

    # Address bar based features
    features.append(havingIP(url))
    features.append(haveAtSign(url))
    features.append(getLength(url))
    features.append(getDepth(url))
    features.append(redirection(url))
    features.append(httpDomain(url))
    features.append(tinyURL(url))
    features.append(prefixSuffix(url))

    # Domain based features
    dns = 0
    try:
        domain_name = whois.whois(urlparse(url).netloc)
    except:
        dns = 1

    features.append(dns)
    features.append(web_traffic(url))
    features.append(1 if dns == 1 else domainAge(domain_name))
    features.append(1 if dns == 1 else domainEnd(domain_name))

    # HTML & Javascript based features
    try:
        response = requests.get(url, timeout=10)
    except requests.exceptions.RequestException:
        response = ""

    features.append(iframe(response))
    features.append(mouseOver(response))
    features.append(rightClick(response))
    features.append(forwarding(response))

    return features

# List of URLs for prediction
urls = [
    "https://www.hdfcbank.com/",
    "http://ww1.natwesti.com/lander",
    "https://www.icicibank.com/",
    "https://www.onlinesbi.sbi/",
    "http://ww1.barclays-supports.com/?subid1=ee4f6d1b-1cba-11ef-b69b-2d15a13d704a",
    "https://www.wellsfargo.com",
    "https://get.onsafesearch.com/view/item_89681.html",
    "https://www.bankofamerica.com",
    "bigwoodsbar.com",
    "http://www.example.net",
    "http://moodle.mitaoe.ac.in/login/index.php",
    "http://www.tesla-fbaqr\.net",
    "http://www.au-sintlink\.zip\.net"

    # Add more URLs here
]

# Extract features for each URL
features = [featureExtraction(url) for url in urls]

# Convert to DataFrame
feature_names = ['Have_IP', 'Have_At', 'URL_Length', 'URL_Depth', 'Redirection',
                 'https_Domain', 'TinyURL', 'Prefix_Suffix', 'DNS_Record',
                 'Web_Traffic', 'Domain_Age', 'Domain_End', 'iFrame',
                 'Mouse_Over', 'Right_Click', 'Web_Forwards']

df = pd.DataFrame(features, columns=feature_names)

# Load the scaler and model
scaler = StandardScaler()
loaded_model = load_model('seq2seq_model.h5')

# Standardize the features
df_scaled = scaler.fit_transform(df)

# Reshape data for the model [samples, timesteps, features]
df_reshaped = df_scaled.reshape((df_scaled.shape[0], df_scaled.shape[1], 1))

# Create initial decoder input sequence
# Assuming the decoder input is the same shape as the encoder input but starts with a zero sequence
decoder_input = np.zeros_like(df_reshaped)

# Make predictions
predictions = loaded_model.predict([df_reshaped, decoder_input])

# Debugging: Check the shape and content of predictions
# print("Predictions shape:", predictions.shape)
# print("Predictions content:", predictions)

# Process the predictions to get a single label per URL
# Here, we take the mean of the predictions across the 16 time steps
predicted_labels = (predictions.mean(axis=1) > 0.2).astype(int)

# Output results
for url, label in zip(urls, predicted_labels):
    print(f'URL: {url} - Predicted Label: {"Phishing" if label == 1 else "Legitimate"}')

# import pickle

# # Save the scaler object as a pickle file
# with open('scaler.pkl', 'wb') as f:
#     pickle.dump(scaler, f)

#time distributed

# List of URLs for prediction
urls = [
    "https://www.hdfcbank.com/",
    "http://ww1.natwesti.com/lander",
    "https://www.icicibank.com/",
    "https://www.onlinesbi.sbi/",
    "http://ww1.barclays-supports.com/?subid1=ee4f6d1b-1cba-11ef-b69b-2d15a13d704a",
    "https://www.wellsfargo.com",
    "https://get.onsafesearch.com/view/item_89681.html",
    "https://www.bankofamerica.com",
    "bigwoodsbar.com",
    "http://www.example.net",
    "http://moodle.mitaoe.ac.in/login/index.php",
    "http://www.tesla-fbaqr\.net",
    "http://www.au-sintlink\.zip\.net"

    # Add more URLs here
]

# Extract features for each URL
features = [featureExtraction(url) for url in urls]

# Convert to DataFrame
feature_names = ['Have_IP', 'Have_At', 'URL_Length', 'URL_Depth', 'Redirection',
                 'https_Domain', 'TinyURL', 'Prefix_Suffix', 'DNS_Record',
                 'Web_Traffic', 'Domain_Age', 'Domain_End', 'iFrame',
                 'Mouse_Over', 'Right_Click', 'Web_Forwards']

df = pd.DataFrame(features, columns=feature_names)

# Load the scaler and model
scaler = StandardScaler()
loaded_model1 = load_model('timedistributed_lstm_model.h5')

# Standardize the features
df_scaled = scaler.fit_transform(df)

# Reshape data for the model [samples, timesteps, features]
df_reshaped = df_scaled.reshape((df_scaled.shape[0], df_scaled.shape[1], 1))

# Make predictions
predictions = loaded_model1.predict(df_reshaped)

# Debugging: Check the shape and content of predictions
# print("Predictions shape:", predictions.shape)
# print("Predictions content:", predictions)

# Process the predictions to get a single label per URL
# Here, we take the mean of the predictions across the 16 time steps
predicted_labels = (predictions.mean(axis=1) > 0.2).astype(int)

# Output results
for url, label in zip(urls, predicted_labels):
    print(f'URL: {url} - Predicted Label: {"Phishing" if label == 1 else "Legitimate"}')

